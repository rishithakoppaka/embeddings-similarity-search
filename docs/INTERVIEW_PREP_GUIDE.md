# Interview Prep Guide: Embeddings & Vector Search

## Table of Contents
1. [Core Concepts](#core-concepts)
2. [Technical Deep Dives](#technical-deep-dives)
3. [Implementation Questions](#implementation-questions)
4. [System Design Questions](#system-design-questions)
5. [Code Walkthrough](#code-walkthrough)
6. [Common Interview Questions](#common-interview-questions)
7. [Best Practices](#best-practices)
8. [Troubleshooting Scenarios](#troubleshooting-scenarios)

---

## Core Concepts

### 1. What are Embeddings?

**Answer:**
Embeddings are dense numerical representations of text (or other data) in a high-dimensional vector space. They capture semantic meaning - similar texts have similar vectors.

**Key Points:**
- Convert text â†’ fixed-size vector (e.g., 384, 768, 1536 dimensions)
- Preserve semantic relationships
- Generated by neural networks trained on large text corpora
- Similar texts = vectors close in vector space

**Example:**
```
"Machine learning" â†’ [0.023, -0.145, 0.892, ..., 0.567]
"AI algorithms"    â†’ [0.019, -0.142, 0.889, ..., 0.564]  (similar!)
"Cooking recipes"  â†’ [-0.234, 0.456, -0.123, ..., 0.234]  (different)
```

### 2. Why Use Embeddings?

**Answer:**
- **Semantic Search**: Find meaningfully similar content, not just keyword matches
- **Machine Learning**: Input format for ML models
- **Clustering**: Group similar items
- **Recommendations**: Find similar items
- **RAG Systems**: Retrieve relevant context for LLMs

### 3. Cosine Similarity Explained

**Definition:**
Measures the cosine of the angle between two vectors, indicating direction similarity (not magnitude).

**Formula:**
```
cosine_similarity = (A Â· B) / (||A|| Ã— ||B||)
```

**Properties:**
- Range: -1 to 1
- 1.0 = Identical direction (same meaning)
- 0.0 = Orthogonal (unrelated)
- -1.0 = Opposite direction

**Why Cosine, not Euclidean?**
- Cosine focuses on direction (semantic meaning)
- Euclidean focuses on magnitude (distance)
- For embeddings, direction matters more than magnitude

---

## Technical Deep Dives

### 4. Embedding Models Comparison

| Model | Provider | Dimensions | Speed | Quality | Cost |
|-------|----------|------------|-------|---------|------|
| text-embedding-3-small | OpenAI | 1536 | Fast | High | Paid |
| text-embedding-ada-002 | OpenAI | 1536 | Medium | Medium | Paid |
| embedding-001 | Gemini | 768 | Fast | High | Paid |
| Mock | Local | 384 | Very Fast | None | Free |

**Interview Tip:** Know trade-offs:
- Higher dimensions = better quality but slower
- Lower dimensions = faster but less nuanced

### 5. Vector Database Architecture

**Components:**
1. **Indexing**: HNSW (Hierarchical Navigable Small World) - approximate nearest neighbor
2. **Storage**: Persistent storage for vectors and metadata
3. **Query Engine**: Fast similarity search
4. **API Layer**: REST/gRPC interfaces

**Why Vector DBs?**
- **Scalability**: Handle millions of vectors
- **Speed**: Optimized indexes (sub-linear search)
- **Persistence**: Data survives restarts
- **Features**: Filtering, metadata, distributed search

### 6. HNSW Algorithm (Qdrant uses this)

**Concept:**
- Multi-layer graph structure
- Each layer is a subset of the layer below
- Search starts at top layer, navigates down
- Time complexity: O(log N) for search

**Interview Question:** "How does HNSW work?"
**Answer:**
1. Build multi-layer graph (fewer nodes at top)
2. Start search at top layer (few nodes to check)
3. Navigate to nearest neighbor
4. Move to next layer, refine search
5. Continue until bottom layer
6. Return k nearest neighbors

---

## Implementation Questions

### 7. Implement Cosine Similarity from Scratch

**Expected Code:**
```python
import numpy as np

def cosine_similarity(vec1, vec2):
    """
    Calculate cosine similarity between two vectors.
    
    Args:
        vec1: numpy array
        vec2: numpy array
    
    Returns:
        float: similarity score between -1 and 1
    """
    # Check dimensions match
    if vec1.shape != vec2.shape:
        raise ValueError("Vectors must have same dimensions")
    
    # Dot product
    dot_product = np.dot(vec1, vec2)
    
    # L2 norms
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    
    # Handle zero vectors
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    # Cosine similarity
    similarity = dot_product / (norm1 * norm2)
    
    return float(similarity)
```

**Follow-up Questions:**
- "What if vectors are already normalized?" â†’ Just dot product
- "How to optimize for batch operations?" â†’ Vectorized NumPy operations
- "What's the time complexity?" â†’ O(d) where d is dimension

### 8. Implement Top-K Search

**Expected Code:**
```python
def find_top_k_similar(query_vector, candidate_vectors, k=3):
    """
    Find top-k most similar vectors.
    
    Args:
        query_vector: Query embedding
        candidate_vectors: List of candidate embeddings
        k: Number of results
    
    Returns:
        List of (index, similarity_score) tuples
    """
    # Calculate similarities (vectorized)
    similarities = []
    for i, candidate in enumerate(candidate_vectors):
        sim = cosine_similarity(query_vector, candidate)
        similarities.append((i, sim))
    
    # Sort by similarity (descending)
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    # Return top-k
    return similarities[:k]
```

**Optimization:**
```python
# Vectorized version (faster)
def find_top_k_vectorized(query, candidates, k=3):
    # Convert to numpy array
    candidates_matrix = np.array(candidates)
    query_vec = np.array(query)
    
    # Batch dot products
    dot_products = np.dot(candidates_matrix, query_vec)
    
    # Batch norms
    query_norm = np.linalg.norm(query_vec)
    candidate_norms = np.linalg.norm(candidates_matrix, axis=1)
    
    # Batch similarities
    similarities = dot_products / (query_norm * candidate_norms)
    
    # Get top-k indices
    top_k_indices = np.argsort(similarities)[::-1][:k]
    
    return [(idx, float(similarities[idx])) for idx in top_k_indices]
```

### 9. Embedding Generation Pipeline

**Interview Question:** "Walk me through generating embeddings for 1000 documents."

**Answer:**
```python
def generate_embeddings_pipeline(documents, batch_size=100):
    """
    Efficient embedding generation pipeline.
    """
    embeddings = []
    
    # Batch processing
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i + batch_size]
        
        # API call (OpenAI supports batch)
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=batch  # Send multiple at once
        )
        
        # Extract embeddings
        batch_embeddings = [item.embedding for item in response.data]
        embeddings.extend(batch_embeddings)
        
        # Rate limiting
        time.sleep(0.1)  # Avoid hitting rate limits
    
    return embeddings
```

**Key Points:**
- Batch processing (efficient)
- Rate limiting (avoid API errors)
- Error handling (retry logic)
- Progress tracking

---

## System Design Questions

### 10. Design a Semantic Search System

**Requirements:**
- 10 million documents
- Sub-second search latency
- Support filtering by metadata
- Handle 1000 queries/second

**Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Serverâ”‚â”€â”€â”€â”€â†’â”‚  Embedding   â”‚
â”‚  (FastAPI) â”‚     â”‚  Generator   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector DB  â”‚
â”‚  (Qdrant)   â”‚
â”‚  - HNSW     â”‚
â”‚  - Sharding â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Components:**
1. **Embedding Service**: Generate query embeddings
2. **Vector Database**: Store and search 10M vectors
3. **API Layer**: Handle requests, rate limiting
4. **Caching**: Cache frequent queries
5. **Load Balancer**: Distribute load

**Scaling:**
- **Horizontal**: Multiple Qdrant instances (sharding)
- **Caching**: Redis for query results
- **CDN**: Cache static content
- **Read Replicas**: Separate read/write

### 11. Optimize for Large-Scale Search

**Interview Question:** "How would you optimize search for 100 million vectors?"

**Answer:**

1. **Indexing Strategy:**
   - Use HNSW with optimal parameters
   - Tune `ef_construction` and `m` parameters
   - Pre-build indexes

2. **Sharding:**
   - Partition vectors across multiple nodes
   - Hash-based or range-based sharding
   - Query all shards, merge results

3. **Caching:**
   - Cache frequent queries
   - Cache top-k results
   - Use LRU eviction

4. **Approximate Search:**
   - Trade accuracy for speed
   - Use `ef` parameter in HNSW
   - Early termination

5. **Hardware:**
   - SSD storage (faster I/O)
   - More RAM (keep index in memory)
   - GPU acceleration (if supported)

### 12. Handle Embedding Updates

**Question:** "How do you handle updates when embeddings change?"

**Answer:**

**Strategy 1: Re-indexing**
- Periodically regenerate all embeddings
- Rebuild index
- Downtime during rebuild

**Strategy 2: Incremental Updates**
- Update only changed documents
- Upsert in vector DB
- Index auto-updates (Qdrant supports this)

**Strategy 3: Versioning**
- Keep multiple versions
- Query latest version
- Gradual migration

**Best Practice:**
- Use incremental updates for small changes
- Full re-indexing for major model changes
- Blue-green deployment for zero downtime

---

## Code Walkthrough

### 13. Explain This Code

```python
def cosine_similarity_batch(query_vector, candidate_vectors):
    candidates_matrix = np.array(candidate_vectors)
    query_vector = np.array(query_vector).flatten()
    
    dot_products = np.dot(candidates_matrix, query_vector)
    query_norm = np.linalg.norm(query_vector)
    candidate_norms = np.linalg.norm(candidates_matrix, axis=1)
    
    denominator = query_norm * candidate_norms
    denominator[denominator == 0] = 1.0
    
    similarities = dot_products / denominator
    return similarities
```

**Explanation:**
1. **Convert to matrix**: Stack vectors into 2D array
2. **Flatten query**: Ensure 1D shape
3. **Vectorized dot product**: Calculate all dot products at once
4. **Batch norms**: Calculate all norms in one operation
5. **Handle zeros**: Avoid division by zero
6. **Vectorized division**: Calculate all similarities

**Time Complexity:** O(nÃ—d) where n=vectors, d=dimensions
**Space Complexity:** O(nÃ—d) for matrix

### 14. Qdrant Integration Pattern

```python
class QdrantManager:
    def __init__(self, url="localhost", port=6333):
        self.client = QdrantClient(host=url, port=port)
    
    def insert_vectors(self, collection, vectors, texts):
        points = [
            PointStruct(
                id=i,
                vector=vec.tolist(),
                payload={"text": text}
            )
            for i, (vec, text) in enumerate(zip(vectors, texts))
        ]
        self.client.upsert(collection, points)
    
    def search(self, collection, query_vector, top_k=3):
        results = self.client.query_points(
            collection_name=collection,
            query=query_vector,
            limit=top_k
        )
        return results
```

**Key Points:**
- Connection management
- Batch insertion
- Payload for metadata
- Query interface

---

## Common Interview Questions

### 15. "What's the difference between cosine similarity and Euclidean distance?"

**Answer:**
- **Cosine Similarity**: Measures angle (direction), range -1 to 1
- **Euclidean Distance**: Measures straight-line distance, range 0 to âˆ

**When to use:**
- **Cosine**: Text embeddings (direction = meaning)
- **Euclidean**: When magnitude matters (e.g., image pixels)

**Example:**
```python
vec1 = [1, 1]  # Magnitude: âˆš2
vec2 = [2, 2]  # Magnitude: 2âˆš2

# Cosine: 1.0 (same direction)
# Euclidean: âˆš2 (different magnitude)
```

### 16. "How do you choose embedding dimensions?"

**Answer:**
- **Lower (128-384)**: Fast, good for simple tasks
- **Medium (768)**: Balanced (Gemini default)
- **Higher (1536)**: Better quality, slower (OpenAI)

**Factors:**
- Task complexity
- Dataset size
- Latency requirements
- Cost constraints

**Rule of thumb:** Start with 384, increase if quality insufficient

### 17. "What happens if you normalize vectors before cosine similarity?"

**Answer:**
- Normalized vectors have ||v|| = 1
- Cosine similarity = dot product (faster!)
- No division needed

**Optimization:**
```python
# Normalize once
normalized_vectors = [v / np.linalg.norm(v) for v in vectors]

# Then cosine similarity is just:
similarity = np.dot(query, candidate)  # Much faster!
```

### 18. "How do you handle out-of-vocabulary words in embeddings?"

**Answer:**
- **Subword tokenization**: Break into subwords (BERT, GPT)
- **Character-level**: Embed characters
- **Fallback**: Use average of similar words
- **Context-aware**: Use surrounding words (contextual embeddings)

### 19. "What's the difference between dense and sparse embeddings?"

**Answer:**

**Dense Embeddings:**
- All dimensions have values
- Fixed size (e.g., 384 dimensions)
- Example: Word2Vec, BERT

**Sparse Embeddings:**
- Mostly zeros
- Variable size
- Example: TF-IDF, BM25

**Use Cases:**
- **Dense**: Semantic similarity, neural networks
- **Sparse**: Keyword matching, traditional IR

### 20. "How do you evaluate embedding quality?"

**Answer:**

**Metrics:**
1. **Downstream Tasks**: Classification accuracy
2. **Similarity Tasks**: Correlation with human judgments
3. **Clustering**: Silhouette score
4. **Retrieval**: Precision@K, Recall@K

**Datasets:**
- STS (Semantic Textual Similarity)
- GLUE benchmark
- Custom domain-specific tests

---

## Best Practices

### 21. Embedding Best Practices

1. **Batch Processing**: Process multiple texts at once
2. **Caching**: Cache embeddings to avoid regeneration
3. **Normalization**: Normalize for faster cosine similarity
4. **Error Handling**: Retry logic for API calls
5. **Rate Limiting**: Respect API limits
6. **Versioning**: Track embedding model versions

### 22. Vector Database Best Practices

1. **Index Tuning**: Optimize HNSW parameters
2. **Batch Inserts**: Insert in batches (100-1000)
3. **Metadata**: Store text with vectors (payload)
4. **Filtering**: Use metadata filters when possible
5. **Monitoring**: Track query latency, accuracy
6. **Backup**: Regular backups of vector data

### 23. Production Considerations

1. **Scalability**: Plan for growth
2. **Latency**: Target <100ms for search
3. **Availability**: Redundancy, failover
4. **Cost**: Optimize API calls, caching
5. **Security**: Encrypt sensitive data
6. **Monitoring**: Logs, metrics, alerts

---

## Troubleshooting Scenarios

### 24. "Search returns irrelevant results"

**Possible Causes:**
- Wrong embedding model
- Low-quality embeddings
- Incorrect similarity metric
- Need to fine-tune model

**Solutions:**
- Try different embedding model
- Increase embedding dimensions
- Use domain-specific embeddings
- Fine-tune on your data

### 25. "Search is too slow"

**Possible Causes:**
- Too many vectors
- No indexing
- Sequential processing
- Network latency

**Solutions:**
- Use vector database (Qdrant)
- Optimize index parameters
- Batch processing
- Caching frequent queries
- Approximate search (trade accuracy)

### 26. "Out of memory errors"

**Possible Causes:**
- Loading all vectors into memory
- Large embedding dimensions
- No batching

**Solutions:**
- Use vector database
- Process in batches
- Stream processing
- Reduce embedding dimensions
- Use disk-based storage

---

## Quick Reference

### Formulas

**Cosine Similarity:**
```
cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)
```

**L2 Norm:**
```
||v|| = âˆš(vâ‚Â² + vâ‚‚Â² + ... + vâ‚™Â²)
```

**Dot Product:**
```
A Â· B = Î£(aáµ¢ Ã— báµ¢)
```

### Time Complexities

- Cosine similarity: O(d) where d = dimensions
- Batch cosine: O(nÃ—d) where n = vectors
- HNSW search: O(log n) approximate
- Linear search: O(nÃ—d)

### Common Dimensions

- OpenAI: 1536
- Gemini: 768
- BERT: 768
- Word2Vec: 300
- Mock: 384

---

## Practice Problems

### Problem 1: Implement Batch Similarity
Write a function that calculates cosine similarity between one query vector and N candidate vectors efficiently.

### Problem 2: Top-K with Filtering
Implement top-k search with metadata filtering (e.g., only search in certain categories).

### Problem 3: Embedding Pipeline
Design a system that:
- Ingests documents
- Generates embeddings
- Stores in vector DB
- Handles updates
- Supports search

### Problem 4: Optimize Search
Given 1 million vectors, how would you optimize search to <50ms latency?

---

## Final Tips

1. **Understand the math**: Know cosine similarity formula
2. **Know trade-offs**: Speed vs accuracy, dimensions vs quality
3. **Practice coding**: Implement from scratch
4. **System design**: Think about scale, latency, availability
5. **Real examples**: Be ready to discuss real use cases
6. **Ask questions**: Clarify requirements before answering

**Good luck with your interview! ğŸš€**

